{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkTrial').master('local').getOrCreate()\n",
    "sc= spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n",
      "+---------------------------------------------------------------------------------------------------------------+-----+\n",
      "|text                                                                                                           |label|\n",
      "+---------------------------------------------------------------------------------------------------------------+-----+\n",
      "|Sorry, I'll call later in meeting                                                                              |0    |\n",
      "|Dont worry. I guess he's busy.                                                                                 |0    |\n",
      "|Call FREEPHONE 0800 542 0578 now!                                                                              |1    |\n",
      "|Win a 1000 cash prize or a prize worth 5000                                                                    |1    |\n",
      "|Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...|0    |\n",
      "+---------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([StructField('SR',IntegerType()),\n",
    "             StructField('text', StringType()),\n",
    "             StructField('label', IntegerType())])\n",
    "\n",
    "sms = spark.read.csv('file:///D:/Ravi_Data/Pyspark/sms.csv',header= False,sep = \";\" , schema = schema, nullValue = \"NA\",)\n",
    "print(sms.count())\n",
    "sms = sms.drop(\"SR\")\n",
    "sms.show(5,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', 'string'), ('label', 'int')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----+------------------------------------------+\n",
      "|text                              |label|words                                     |\n",
      "+----------------------------------+-----+------------------------------------------+\n",
      "|Sorry I'll call later in meeting  |0    |[sorry, i'll, call, later, in, meeting]   |\n",
      "|Dont worry I guess he's busy      |0    |[dont, worry, i, guess, he's, busy]       |\n",
      "|Call FREEPHONE now                |1    |[call, freephone, now]                    |\n",
      "|Win a cash prize or a prize worth |1    |[win, a, cash, prize, or, a, prize, worth]|\n",
      "+----------------------------------+-----+------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary functions\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Remove punctuation (REGEX provided) and numbers\n",
    "wrangled = sms.withColumn('text', regexp_replace(sms.text,'[_():;,.!?\\\\-]',' '))\n",
    "wrangled = wrangled.withColumn('text',regexp_replace(wrangled.text,'[0-9]',' '))\n",
    "\n",
    "# Merge multiple spaces\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
    "\n",
    "# Split the text into words\n",
    "wrangled = Tokenizer(inputCol='text', outputCol='words').transform(wrangled)\n",
    "\n",
    "wrangled.show(4,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|                text|label|               words|               terms|                hash|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|Sorry I'll call l...|    0|[sorry, i'll, cal...|[sorry, call, lat...|(5000,[20,146,560...|\n",
      "|Dont worry I gues...|    0|[dont, worry, i, ...|[dont, worry, gue...|(5000,[2977,3343,...|\n",
      "| Call FREEPHONE now |    1|[call, freephone,...|   [call, freephone]|(5000,[146,1957],...|\n",
      "|Win a cash prize ...|    1|[win, a, cash, pr...|[win, cash, prize...|(5000,[1863,2213,...|\n",
      "|Go until jurong p...|    0|[go, until, juron...|[go, jurong, poin...|(5000,[98,740,750...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover,HashingTF, IDF\n",
    "\n",
    "# Remove stop words.\n",
    "wrangled = StopWordsRemover(inputCol='words', outputCol='terms').transform(wrangled)\n",
    "\n",
    "# Apply the hashing trick\n",
    "wrangled = HashingTF(inputCol='terms', outputCol='hash', numFeatures=5000).transform(wrangled)  \n",
    "wrangled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|        (5000,[],[])|\n",
      "|    0|        (5000,[],[])|\n",
      "|    1|(5000,[15,581,102...|\n",
      "|    0|(5000,[146,1388,3...|\n",
      "|    1|(5000,[592,3372,4...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "train, test = wrangled.randomSplit([0.8,0.2],seed=111)\n",
    "\n",
    "# Convert hashed symbols to TF-IDF\n",
    "tf_idf = IDF(inputCol='hash',outputCol='features').fit(train)\n",
    "\n",
    "train = tf_idf.transform(train).select('label','features')\n",
    "test = tf_idf.transform(test).select('label','features')\n",
    "\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|    0|        (5000,[],[])|[23.6961549531267...|[0.99999999994884...|       0.0|\n",
      "|    0|(5000,[20,146,138...|[51.9045954068155...|[1.0,2.8715780792...|       0.0|\n",
      "|    1|(5000,[139,188,22...|[37.4962669107522...|[1.0,5.1949119226...|       0.0|\n",
      "|    1|(5000,[1233,1397,...|[-46.401831312161...|[7.04596872142492...|       1.0|\n",
      "|    1|(5000,[122,146,25...|[35.0853059481375...|[0.99999999999999...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit a Logistic Regression model to the training data\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr_model = LogisticRegression(labelCol=\"label\", featuresCol=\"features\").fit(train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "prediction = lr_model.transform(test)\n",
    "prediction.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy decision tree : 0.9709355131698456\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import evaluation\n",
    "evaluator = evaluation.MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "print(\"Accuracy decision tree :\",evaluator.evaluate(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
